Song, Chaoming, et al. "Limits of predictability in human mobility." Science 327.5968 (2010): 1018-1021.
      -Human movement is predictable
      -measure entropy of trajectory
      -Entropy: most fundamental quantity capturing degree of predictability of a time series [24]
      -measuring predictability: PI
        -fano's inequality
        -if user w entropy S moves between N locations, then PI < PI^{max}(S<N)
            -PI^Max = S = H(PI^max)log2(N-1) w/binary entropy fn H(PI^max) = -PI^max log2(PI^max)-(1-PI^max)log2(1-PI_max)
            -for user with PI^max = .2, 80% of time the individual chooses location in random fashion (only predictable 20% of the time)
	    -regularity: probability of finding user in most visited location during a given hour
	    		 -lower pound on predictability PI
			 -ignores temporal correlations

Cover, Thomas M., and Joy A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
	-Entropy = H(X) = -SUM_x {p(x)log(p(x))}
	-I(X,Y) = mutual information, relative entropy between join distributions and product distribution p(x)p(y)
	-Fano's inequality: related entropy and error of prediction
		-Pe = probability of error = P(X_c != X} (X_c = estimate of X)
		-H(Pe) + Pe*log*|L| - 1) >= H(X|Y) L = alphabet of random var
	-conditioning reduced entropy
	-for markov chains
		-relative entropy D(\mu_n || \mu_n') reduces with time
		-Entropy H(X_n) increases if stationary ditribution is uniform
		-conditional entropy H(X_0|Y_0) of increases with time for stationary markov chain
	-markov chain X->Y->Z
		-p(x,y,z) = p(x)p(y|x)p(z|y)
		-I(X;Y) >= I(X;Z)
    ---Entropy of English---
        -ergodic: can deduce properties from a suffiently long sample
    -second+order word model: transition probabilities match english
	
MDL slides
    -how to measure regularity?
    -kolmogorov complexity: length of shorted program that prints the sequence
    -invariance: length of two shortest programs in distrinct languages differ by no more than length C.
    -MDL principle:
        -f if you have data X and machine M, you can ancode X with M
        -can encode M
        -complete code is L(X,M)
        -model M* best describing D is L(X,M*) with shortest length
        -MDL principle: L(X,M) = L(X|M) + L(M) (+ = concatonation)
            -L(X|M) = data encoded with benifit of M
            -L(M) = encoded model
        -posterior density for m, data X:
            -g(m|X) = g(X|m)g(m)
            -g(X|m) is data conditional on model, g(m) is prior density
    -ngrams are 0-1 nxn matrix
        -can partition into blocks (submatrices consisting of 1's)
        -code graph by identifying all blocks
            -identify edges "sides" of block


Bartholomew, David J. "Errors of prediction for Markov chain models." Journal of the Royal Statistical Society. Series B (Methodological) (1975): 444-456.
    -statistical error- error inherent to the probability of moving from one state to another (what we want)
    -estimation error- we have to estimate what state we're in and sometimes we're wrong
    -specification error- assumptions of model may not hold in practice
